{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stress-testing orbital with GBMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "\n",
    "GBMs are a more complicated tree-based ensemble model than RandomForests insomuchas their predictions are not simply the \"majority vote\" of their leaf nodes. We did, in fact, go to all the trouble of boosting those gradients...\n",
    "\n",
    "Instead, GBM predictions are calculated as `e^x / (1 + e^x)` or more simply `1 / (1 + e^-x)` where x is the result of summing the outputs of the trees (subject to the correct dampening parameters). This makes them a bit trickier for `orbital` to get right. \n",
    "\n",
    "Currently, this transformation is done correctly for `output_probabiliy.1` but incorrectly for the other variables `orbital` offers. Fortunately, the correct one is the one that practicioners will want 90% of the time, but there could be other cases like modeling propensity scores for an observational study where both positive and negative prediction fields are used. \n",
    "\n",
    "I opened an issue about this in the `orbital` repo. You can track it [here](https://github.com/posit-dev/orbital/issues/53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbital\n",
    "import duckdb\n",
    "import sqlglot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emily\\Desktop\\orbital-exploration\\.venv\\Lib\\site-packages\\orbital\\translation\\steps\\trees\\classifier.py:135: FutureWarning: `case` is deprecated as of v10.0.0, removed in v11.0; use ibis.cases()\n",
      "  ibis.case().when(condition, t_val).else_(f_val).end()\n",
      "c:\\Users\\emily\\Desktop\\orbital-exploration\\.venv\\Lib\\site-packages\\orbital\\translation\\steps\\trees\\classifier.py:157: FutureWarning: `case` is deprecated as of v10.0.0, removed in v11.0; use ibis.cases()\n",
      "  ibis.case()\n"
     ]
    }
   ],
   "source": [
    "#| label: setup\n",
    "\n",
    "# mock data\n",
    "X_train, y_train = make_classification(random_state=504)\n",
    "X_train = X_train.round(3)\n",
    "\n",
    "# mock pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"prep\", ColumnTransformer([(\"scaler\", StandardScaler(), [])], remainder=\"passthrough\")),\n",
    "    (\"gbm\", GradientBoostingClassifier(max_depth = 1, n_estimators = 1, random_state=102)),\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# render SQL from orbital\n",
    "n_cols = len(X_train[0])\n",
    "nm_cols = [f\"var_{i}\" for i in range(n_cols)]\n",
    "feat_dict = {}\n",
    "for n in nm_cols:\n",
    "    feat_dict[n] = orbital.types.DoubleColumnType()\n",
    "orbital_pipeline = orbital.parse_pipeline(pipeline, features=feat_dict)\n",
    "sql_raw = orbital.export_sql(\"DATA_TABLE\", orbital_pipeline, dialect=\"duckdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolating the incorrect outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facially, we can see something isn't right because the output probabilities don't sum to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   output_label  output_probability.0  output_probability.1\n",
      "0             0              0.693682              0.545526\n",
      "1             0              0.693682              0.545526\n",
      "2             0              0.693682              0.545526\n",
      "3             0              0.693682              0.545526\n",
      "4             0              0.693682              0.545526\n",
      "   output_label  output_probability.0  output_probability.1\n",
      "0             0              0.693682              0.545526\n",
      "1             0              0.693682              0.545526\n",
      "2             0              0.693682              0.545526\n",
      "3             0              0.693682              0.545526\n",
      "4             0              0.693682              0.545526\n"
     ]
    }
   ],
   "source": [
    "#| label: valid-struct\n",
    "DATA_TABLE = pd.DataFrame(X_train, columns = nm_cols)\n",
    "\n",
    "# structure of table\n",
    "df_preds = duckdb.sql(sql_raw).df()\n",
    "print(df_preds.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which value (or both) is wrong? The positive prediction case seems to be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl and orb match: True\n",
      "ppl and orb match: True\n"
     ]
    }
   ],
   "source": [
    "#| label: valid-values\n",
    "preds_orb = df_preds['output_probability.1']\n",
    "preds_ppl     = pipeline.predict_proba(X_train)[:,1]\n",
    "print(f\"ppl and orb match: {np.all(np.isclose(preds_ppl, preds_orb))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get some insight by looking at the underlying query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT\n",
      "  CAST(CASE\n",
      "    WHEN CASE\n",
      "      WHEN \"t0\".\"var_6\" <= 0.3644999861717224\n",
      "      THEN -0.15555556118488312\n",
      "      ELSE 0.1826086938381195\n",
      "    END > 0.5\n",
      "    THEN 1\n",
      "    ELSE 0\n",
      "  END AS BIGINT) AS \"output_label\",\n",
      "  1 / (\n",
      "    EXP(\n",
      "      -(\n",
      "        1.0 - CASE\n",
      "          WHEN \"t0\".\"var_6\" <= 0.3644999861717224\n",
      "          THEN -0.15555556118488312\n",
      "          ELSE 0.1826086938381195\n",
      "        END\n",
      "      )\n",
      "    ) + 1\n",
      "  ) AS \"output_probability.0\",\n",
      "  1 / (\n",
      "    EXP(\n",
      "      -CASE\n",
      "        WHEN \"t0\".\"var_6\" <= 0.3644999861717224\n",
      "        THEN -0.15555556118488312\n",
      "        ELSE 0.1826086938381195\n",
      "      END\n",
      "    ) + 1\n",
      "  ) AS \"output_probability.1\"\n",
      "FROM \"DATA_TABLE\" AS \"t0\"\n",
      "SELECT\n",
      "  CAST(CASE\n",
      "    WHEN CASE\n",
      "      WHEN \"t0\".\"var_6\" <= 0.3644999861717224\n",
      "      THEN -0.15555556118488312\n",
      "      ELSE 0.1826086938381195\n",
      "    END > 0.5\n",
      "    THEN 1\n",
      "    ELSE 0\n",
      "  END AS BIGINT) AS \"output_label\",\n",
      "  1 / (\n",
      "    EXP(\n",
      "      -(\n",
      "        1.0 - CASE\n",
      "          WHEN \"t0\".\"var_6\" <= 0.3644999861717224\n",
      "          THEN -0.15555556118488312\n",
      "          ELSE 0.1826086938381195\n",
      "        END\n",
      "      )\n",
      "    ) + 1\n",
      "  ) AS \"output_probability.0\",\n",
      "  1 / (\n",
      "    EXP(\n",
      "      -CASE\n",
      "        WHEN \"t0\".\"var_6\" <= 0.3644999861717224\n",
      "        THEN -0.15555556118488312\n",
      "        ELSE 0.1826086938381195\n",
      "      END\n",
      "    ) + 1\n",
      "  ) AS \"output_probability.1\"\n",
      "FROM \"DATA_TABLE\" AS \"t0\"\n"
     ]
    }
   ],
   "source": [
    "sql_fmt = sqlglot.transpile(sql_raw, write=\"duckdb\", identify=True, pretty=True)[0]\n",
    "print(sql_fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see where the logic has broken down. The SQL translation is:\n",
    "\n",
    "- `output_label`: if {tree} > 0.5 then 1 else 0\n",
    "- `output_probability.0`: 1 / (exp(1 - {tree}) + 1)\n",
    "- `output_probability.1`: 1 / (exp(-tree) + 1)\n",
    "\n",
    "Only the last of these correctly post-processes our tree:\n",
    "\n",
    "- `output_probability.1` here is right. This is taking the function and applying the inverse logistic transformation. \n",
    "- `output_probability.0` here is wrong. What we want is `1 - transform(tree)` but instead we get `transform(1 - tree)`. Specifically, I believe the problem is that [this happens](https://github.com/posit-dev/orbital/blob/08302c9b1c403d209d00d7c5974b2fd17b51919b/src/orbital/translation/steps/trees/classifier.py#L166) before [this happens](https://github.com/posit-dev/orbital/blob/08302c9b1c403d209d00d7c5974b2fd17b51919b/src/orbital/translation/steps/linearclass.py#L99).\n",
    "- `output_label` hese is also wrong. It ignores the transformation completely. \n",
    "\n",
    "Put another way, We can see that for output_probability.1 it is correctly applying the sigmoid transformation ( 1/( e^-z + 1) which is equivalent to e^z  / (e^z + 1)) . However, for output_probability.0 it is doing 1 / (e^-(1-z) + 1) instead of something equivalent to 1 / (e^z + 1) which would give the desired result.\n",
    "\n",
    "Needless to say, if I swap in a RandomForestClassifier, this works as expected since no post-transformations are required."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
