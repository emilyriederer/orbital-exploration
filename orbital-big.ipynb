{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing orbital on larger models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "\n",
    "`orbital` translate all model scoring logic to SQL. This is trivial for linear models, but increasingly complex for tree-based and ensemble models. Let's stress test `orbital` with larger and more complex models by scaling up a random forest. \n",
    "\n",
    "We'll train a random forest with 100 trees of depth 10. Since, by default, `RandomForestDefault` is very permissive in terms of small leaf nodes and doesn't do much pruning, this means there could be around 100K (100 * 2^10) decision nodes in the resulting tree logic.\n",
    "\n",
    "To be clear, I don't care about model performance. I just care if this works and how fast it runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbital\n",
    "import duckdb\n",
    "import sqlglot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: model-prep\n",
    "\n",
    "# make data dataset\n",
    "X_train, y_train = make_classification(int(1e6), random_state = 102)\n",
    "X_train = X_train.round(3)\n",
    "\n",
    "# get column names for use in pipeline\n",
    "n_cols = len(X_train[0])\n",
    "nm_cols = [f\"f{i}\" for i in range(n_cols)]\n",
    "feat_dict = {c:orbital.types.DoubleColumnType() for c in nm_cols}\n",
    "\n",
    "# fit sklearn pipeline\n",
    "model_path = \"sample-outputs/big-rfc.joblib\" \n",
    "if os.path.exists(model_path):\n",
    "  pipeline = joblib.load(model_path)\n",
    "else: \n",
    "  pipeline = Pipeline([\n",
    "  (\"pre\", ColumnTransformer([], remainder=\"passthrough\")),\n",
    "  (\"gbm\", RandomForestClassifier(max_depth = 10, n_estimators = 100)),\n",
    "    ])\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  joblib.dump(pipeline, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run `orbital`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emily\\Desktop\\orbital-exploration\\.venv\\Lib\\site-packages\\orbital\\translation\\steps\\trees\\classifier.py:135: FutureWarning: `case` is deprecated as of v10.0.0, removed in v11.0; use ibis.cases()\n",
      "  ibis.case().when(condition, t_val).else_(f_val).end()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emily\\Desktop\\orbital-exploration\\.venv\\Lib\\site-packages\\orbital\\translation\\steps\\trees\\classifier.py:157: FutureWarning: `case` is deprecated as of v10.0.0, removed in v11.0; use ibis.cases()\n",
      "  ibis.case()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 29s\n",
      "Wall time: 3min 41s\n",
      "CPU times: total: 1min 29s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "orbital_pipeline = orbital.parse_pipeline(pipeline, features=feat_dict)\n",
    "sql_raw = orbital.export_sql(\"DATA_TABLE\", orbital_pipeline, dialect=\"duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: cleanup\n",
    "\n",
    "# parse AST from SQL script\n",
    "ast = sqlglot.parse_one(sql_raw)\n",
    "\n",
    "# clean up SQL\n",
    "## drop the class prediction and negative-event predictions\n",
    "ast.expressions[0] = None\n",
    "ast.expressions[1] = None \n",
    "\n",
    "## pretty print -- not important for usage; but we'll take a peak at the output at the end here\n",
    "sql_mod = ast.sql()\n",
    "sql_fmt = sqlglot.transpile(sql_mod, write=\"duckdb\", identify=True, pretty=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47042.333333333336"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count CASEs to gauge size of tree\n",
    "# divide by 3 since raw output repeats logic 3x\n",
    "\n",
    "cases = [match.start() for match in re.finditer('CASE', sql_raw)]\n",
    "len(cases)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Output\n",
    "\n",
    "We can now assess scoring time and double check the validity of our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can see the benefits of cleaning up our SQL to only compute the single predictions column. Both CPU and Wall time is about ~1/2 when we only calculate our positive prediciton. This makes sense because the `orbital` code produces separate (repeated) logic for class prediction, positive probability, and negative probability instead of reusing the computation. That may be fine enough for small problems, but for larger and more complex problems, optimization matters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TABLE = pd.DataFrame(X_train[:1000,], columns = nm_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 16.87 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "4min 46s ± 5min 59s per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "The slowest run took 16.87 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "4min 46s ± 5min 59s per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#| label: scores-raw\n",
    "\n",
    "df_preds = duckdb.sql(sql_raw).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.7 s ± 15.2 s per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "53.7 s ± 15.2 s per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#| label: scores-fmt\n",
    "\n",
    "df_preds = duckdb.sql(sql_fmt).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_preds\u001b[49m\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_preds' is not defined"
     ]
    }
   ],
   "source": [
    "df_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also confirm that our outputs still match in this more complex case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#| label: valid-values\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m preds_orb = \u001b[43mdf_preds\u001b[49m[\u001b[33m'\u001b[39m\u001b[33moutput_probability.1\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[32m      4\u001b[39m preds_ppl = pipeline.predict_proba(DATA_TABLE)[:,\u001b[32m1\u001b[39m]\n",
      "\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mppl and orb match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.all(np.isclose(preds_ppl,\u001b[38;5;250m \u001b[39mpreds_orb))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'df_preds' is not defined"
     ]
    }
   ],
   "source": [
    "#| label: valid-values\n",
    "\n",
    "preds_orb = df_preds['output_probability.1']\n",
    "preds_ppl = pipeline.predict_proba(DATA_TABLE)[:,1]\n",
    "\n",
    "print(f\"ppl and orb match: {np.all(np.isclose(preds_ppl, preds_orb))}\")\n",
    "print(f\"ppl and orb prop mismatch: {sum(~np.isclose(preds_ppl, preds_orb)) / len(preds_ppl)}\")\n",
    "print(f\"ppl and orb corr: {np.corrcoef(preds_ppl, preds_orb)[0][1]:.2f}\")\n",
    "print(f\"ppl and orb MAE: {np.mean(np.abs(preds_ppl - preds_orb)):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Output\n",
    "\n",
    "We can save out this long query to see what it looks like. Spoiler alert: prepare to scroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample-outputs/long_query.sql\", \"w\") as file:\n",
    "    file.write(sql_fmt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
