{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing orbital on larger models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "\n",
    "`orbital` translate all model scoring logic to SQL. This is trivial for linear models, but increasingly complex for tree-based and ensemble models. Let's stress test `orbital` with larger and more complex models by scaling up a random forest. \n",
    "\n",
    "We'll train a random forest with 100 trees of depth 10. Since, by default, `RandomForestDefault` is very permissive in terms of small leaf nodes and doesn't do much pruning, this means there could be around 100K (100 * 2^10) decision nodes in the resulting tree logic.\n",
    "\n",
    "To be clear, I don't care about model performance. I just care if this works and how fast it runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbital\n",
    "import duckdb\n",
    "import sqlglot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: model-prep\n",
    "\n",
    "# make data dataset\n",
    "X_train, y_train = make_classification(int(1e6), random_state = 102)\n",
    "X_train = X_train.round(3)\n",
    "\n",
    "# get column names for use in pipeline\n",
    "n_cols = len(X_train[0])\n",
    "nm_cols = [f\"f{i}\" for i in range(n_cols)]\n",
    "feat_dict = {c:orbital.types.DoubleColumnType() for c in nm_cols}\n",
    "\n",
    "# fit sklearn pipeline\n",
    "model_path = \"sample-outputs/big-rfc.joblib\" \n",
    "if os.path.exists(model_path):\n",
    "  pipeline = joblib.load(model_path)\n",
    "else: \n",
    "  pipeline = Pipeline([\n",
    "  (\"preprocess\", ColumnTransformer([(\"scaler\", StandardScaler(), [])], remainder=\"passthrough\")),\n",
    "  (\"gbm\", RandomForestClassifier(max_depth = 10, n_estimators = 100)),\n",
    "    ])\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  joblib.dump(pipeline, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run `orbital`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "orbital_pipeline = orbital.parse_pipeline(pipeline, features=feat_dict)\n",
    "sql_raw = orbital.export_sql(\"DATA_TABLE\", orbital_pipeline, dialect=\"duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: cleanup\n",
    "\n",
    "# parse AST from SQL script\n",
    "ast = sqlglot.parse_one(sql_raw)\n",
    "\n",
    "# clean up SQL\n",
    "## drop the class prediction and negative-event predictions\n",
    "ast.expressions[0] = None\n",
    "ast.expressions[1] = None \n",
    "\n",
    "## pretty print -- not important for usage; but we'll take a peak at the output at the end here\n",
    "sql_mod = ast.sql()\n",
    "sql_fmt = sqlglot.transpile(sql_mod, write=\"duckdb\", identify=True, pretty=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count CASEs to gauge size of tree\n",
    "# divide by 3 since raw output repeats logic 3x\n",
    "\n",
    "cases = [match.start() for match in re.finditer('CASE', sql_raw)]\n",
    "len(cases)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Output\n",
    "\n",
    "We can now assess scoring time and double check the validity of our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can see the benefits of cleaning up our SQL to only compute the single predictions column. Both CPU and Wall time is about ~1/2 when we only calculate our positive prediciton. This makes sense because the `orbital` code produces separate (repeated) logic for class prediction, positive probability, and negative probability instead of reusing the computation. That may be fine enough for small problems, but for larger and more complex problems, optimization matters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TABLE = pd.DataFrame(X_train[:1000,], columns = nm_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 10\n",
    "\n",
    "#| label: scores-raw\n",
    "\n",
    "df_preds = duckdb.sql(sql_raw).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 10\n",
    "\n",
    "#| label: scores-fmt\n",
    "\n",
    "df_preds = duckdb.sql(sql_fmt).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also confirm that our outputs still match in this more complex case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: valid-values\n",
    "\n",
    "preds_orb = df_preds['output_probability.1']\n",
    "preds_ppl = pipeline.predict_proba(DATA_TABLE)[:,1]\n",
    "\n",
    "print(f\"ppl and orb match: {np.all(np.isclose(preds_ppl, preds_orb))}\")\n",
    "print(f\"ppl and orb prop mismatch: {sum(~np.isclose(preds_ppl, preds_orb)) / len(preds_ppl)}\")\n",
    "print(f\"ppl and orb corr: {np.corrcoef(preds_ppl, preds_orb)[0][1]:.2f}\")\n",
    "print(f\"ppl and orb MAE: {np.mean(np.abs(preds_ppl - preds_orb)):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Output\n",
    "\n",
    "We can save out this long query to see what it looks like. Spoiler alert: prepare to scroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample-outputs/long_query.sql\", \"w\") as file:\n",
    "    file.write(sql_fmt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
